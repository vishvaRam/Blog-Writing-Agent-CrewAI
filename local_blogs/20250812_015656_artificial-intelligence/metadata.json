{
  "title": "Unlocking Scalability: A Deep Dive into Mixture of Experts (MoE) for Modern LLMs",
  "topic": "Artificial Intelligence",
  "created_at": "20250812_015656",
  "word_count": 3661,
  "char_count": 25659,
  "estimated_read_time": 18,
  "status": "draft",
  "platforms": {
    "devto": {
      "published": false,
      "url": ""
    },
    "hashnode": {
      "published": false,
      "url": ""
    },
    "local_saved": true
  }
}